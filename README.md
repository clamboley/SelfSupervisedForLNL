# Self-Supervised Learning applied to Learning with Noisy Labels

PyTorch implementation of SimCLR (https://arxiv.org/abs/2002.05709) and VICReg (https://arxiv.org/abs/2105.04906)
applied for Learning with Noisy Labels on CIFAR10, using DistributedDataParallel for multi-nodes multi-gpus training.

-> Adapted from Bardes, Ponce and Yann LeCun's VICReg implementation.

---

## Pretraining

### Single-node local training

To pretrain SimCLR (or VICReg) with ResNet-18 on a single node with 8 GPUs for 100 epochs, run:

```
python -m torch.distributed.launch --nproc_per_node=8 main.py --data-dir /path/to/cifar10_dataset --exp-dir /path/to/results --method simclr --arch resnet18 --epochs 100 --batch-size 256 -T 0.5 --base-lr 0.2
```

### Multi-node training with SLURM

To pretrain VICReg (or SimCLR) with [submitit](https://github.com/facebookincubator/submitit) and SLURM on 4 nodes with 8 GPUs each for 100 epochs, run:

```
python run_with_submitit.py --nodes 4 --ngpus 8 --data-dir /path/to/cifar10_dataset --exp-dir /path/to/results --method vicreg --arch resnet50 --epochs 100 --batch-size 256 -V 25.0 -I 25.0 -C 1.0 --base-lr 0.2
```


## Evaluation

### Label noise

Label noise configurations are generated in "cifar10_create_label_errors.py". It is asymetric noise generated by
label flipping. We generate noise in (0%, 10%, 20%, 30%, 40%, 50%, 60%, 70%), same for sparsity, with 5 different seeds
using the [Cleanlab](github.com/cleanlab/cleanlab) package.

### Linear evaluation on different noise levels

To evaluate a pretrained ResNet-34 backbone on linear classification on CIFAR10 with 40% noise, 20% sparsity,
generated with seed 0, run:

```
python evaluate.py --data-dir /path/to/cifar10_dataset --exp-dir /path/to/results --method simclr --arch resnet34 --arch-epochs 1000 --loss elr --seed 0 --noise 0.4 --sparsity 0.2
```

Note that the --loss argument gives you the choice between a simple CrossEntropy loss, or the
Early Learning Regularization loss ([ELR](https://arxiv.org/abs/2007.00151))
